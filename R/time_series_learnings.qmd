---
title: "time_series_learnings"
subtitle: DSA301 Time Series Data Analysis
format: html
editor: visual
---

# time_series_01

This file documents my learning process with SMU module DSA301 Time Series Data Analytics. It only covers up to ARIMA as I ran out of time to maintain these codes due to high workload after wk 6. :( 


# Load required packages

```{r}
library(tidyverse)
library(tsibble)
library(fpp2)
library(fpp3) # updated textbook
library(gridExtra)
```

# 1. BoxCox Transformation

### 1.1 The BoxCox transformation is used to make variance over time as constant as possible (constant amplitude and wavelength)

```{r}
autoplot(BoxCox(elec,2))
autoplot(BoxCox(elec,10))
autoplot(BoxCox(elec,0.1))
```

### 1.2 Optimal lambda

```{r}
autoplot(BoxCox(elec, BoxCox.lambda(elec)))
BoxCox.lambda(elec)
```

# 2. Benchmark Forecasting methods

### 2.1 Naive, mean, seasonal naive and drift

Naive methods are *short-term* (eg. predicting the value of today's stock based on latest closing price).

Drift and mean are *long-term.*

```{r}
# Set training data from 1992 to 2006
train <- aus_production |>
  filter_index("1992 Q1" ~ "2006 Q4")
# Fit the models
beer_fit <- train |>
  model(
    Mean = MEAN(Beer),
    `Naïve` = NAIVE(Beer),
    `Seasonal naïve` = SNAIVE(Beer)
  )
# Generate forecasts for 14 quarters
beer_fc <- beer_fit |> forecast(h = 14)
# Plot forecasts against actual values
beer_fc |>
  autoplot(train, level = NULL) +
  autolayer(
    filter_index(aus_production, "2007 Q1" ~ .),
    colour = "black"
  ) +
  labs(
    y = "Megalitres",
    title = "Forecasts for quarterly beer production"
  ) +
  guides(colour = guide_legend(title = "Forecast"))


```

# 3. Ljung-Box test

Tests if time series data is captured in *residuals*, et, where et = yhat - y. If time series data is captured in residuals (et), it is not being captured in the model (yhat). Hence, we have to adjust our model to capture the time series data.

As a rule of thumb, residuals should:

1.  be *uncorrelated*. Correlation means there is information left in residuals to be modelled.

2.  have *zero mean*. Non zero mean in residuals means that fitted values are biased.

```{r}
autoplot(naive(elec,10))
```

### 3.1 Check if residuals are correlated with lagged values of itself

```{r}
forecast(naive(elec,10))
checkresiduals(forecast(naive(elec,10)))
```

### 3.2 There is seasonal data in the residuals, not captured by naive methods.

Hence, we can try seasonal naive forecasting to capture this time series data in our model.

```{r}
forecast(snaive(elec,10))
checkresiduals(forecast(snaive(elec,10)))

```

### 3.3 Seasonality has been captured in the snaive model.

However, there is positive long term trend in the data (seen in the ACF plot).

### 3.4 Ljung-Box takeaway:

A low p-value means that we reject H0 (there is no autocorrelation among lags), hence we assume that H1 is true (autocorrelation exists), failing the Ljung-Box test. Therefore, there is time series information still not captured in the model (since autocorrelation still exists in the **white noise**, which should instead be captured in the **model**)

Basically, if p value is \>0.05, time series model is good enough. if p value \<0.05, the time series model is still inadequate, there is time series data in the residuals yet to be captured by model

**Intuition: h0 = there is no autocorrelation in residuals. We want to accept h0, hence p value must be \>0.05**

# 4. ACF

Using ACF to infer time series patterns (Is there seasonality? Trend?)

Let's look at Australian monthly electricity production data. There is clear trend and seasonality.

```{r}
autoplot(elec)
```

Lets narrow it down to after 1980s, and get the ACF plot.

```{r}
window(elec, start=1980)
ggAcf(window(elec, start=1980), lag=48)
```

## 4.1 How to read ACF plot

The second line represents the autocorrelation between Yt and Yt-1, third line represents autocorrelation between Yt and Yt-2... and so on...

In R, the first line represents autocorrelation of Yt with itself. Naturally, this is 1 (since anything correlated to itself is 1).

## 4.2 What could the ACF plot tell us?

From this acf plot, we are able to infer that the time series has:

**a trend component**: The acf displays a slowly decaying pattern.

Autocorrelations for small lags tend to be large and positive because observations nearby in time are similar in size. Hence, the ACF of trended time series tend to have positive values that slowly decrease as lags increase.

**a seasonal component**: There will be peaks in the acf at the relevant seasonal lags (in this case, since the data is monthly, there are peaks at 12, 24, 36 and 48)

# 5. Stationarity

1.  A stationary time series is one whose moments (1st moment = mean, 2nd moment = variance) do not depend on the time of observation.

2.  A less stringent definition: *Covariance stationarity* - Only the first 2 moments and autocorrelation function are independent of time t, but may still vary with lag interval k.

## 5.1 Why do we wish to estimate models on stationary data?

A stationary data has **parameters which remain constant**. Hence, model's parameters at time t will be good predictor for values after time t.

1.  Is a ts with trend covariance stationary? No, expected values will increase/decrease over time.
2.  Is a white noise ts stationary? Yes, a white noise generally has a mean of 0 and constant variance.
3.  How about a ts with seasonality? No, expected value varies according to month/quarter.
4.  How about a ts with non annual cycles? No, it also has a conditional expectation that varies over time of year.

Takeaway: Stationary data does not have trend or seasonality.

How to make data stationary? Use differencing!

## 5.2 Differencing

### 5.2.1 First differencing

y' = y~t~ - y~t-1~ which returns a series with T-1 values.

diff() function in R

### 5.2.2 Second differencing

y''~t~ = y'~t~ - y'~t-1~ = (y~t~-y~t-1~)-(y~t-1~-y~t-2~) = y~t~-2y~t-1~+y~t-2~ returns a series with T-2 values

diff(diff()) nested function in R

### 5.2.3 Order of integration

If a non-stationary ts is differenced d times before it becomes stationary, then it is integrated of order d!

### 5.2.4 Seasonal differencing

d~m~yt' = yt - y~t-m,~ where m is number of seasons

aka *lag-m differences* eg. m = 4 for quarterly data

diff(,4) in R

## 5.3 Example using elec data

```{r}
autoplot(elec)
```

This is clearly not covariance stationary, lets diff it!

```{r}
autoplot(diff(elec))
```

First difference is still not covariance stationary, mean appears to be 0 but variance increases over time.

Let's try seasonal differencing (since elec data is seasonal anyway).

```{r}
autoplot(diff(elec, 12))
```

Seasonal diff looks good, variance is more stable. But we could do better...

```{r}
autoplot(diff(log(elec),12))

```

Indeed, seasonal diff on log transformed elec is better, but is it stationary?

```{r}
acf((diff(log(elec),12)))

```

Not really stationary... There is a slow drop in acf with lag (suggestive of a trend). For stationarity, ACF would drop to zero relatively quickly.

Lastly, lets try differencing the seasonal differenced log(elec) data.

```{r}
autoplot(diff((diff(log(elec),12))))
acf(diff((diff(log(elec),12))))

```

This is stationary! Time series autoplot is constant in the first and second moments while ACF demonstrates sharp drop.

Takeaway: if data is seasonal, don't do first differencing, just do seasonal differencing first. Also, applying transformations such as logarithms may improve model performance.

## 5.4 Pipeline

1.  Check if a variable is stationary.
2.  If not, apply difference.
3.  Loop and evaluate differenced variable until it passes 'stationarity'.

Note that in reality, we do not difference more than 2 times, as doing so leads to the model losing its economic intuition.

## 5.5 KPSS test for stationarity

Kwiatkowski–Phillips–Schmidt–Shin (KPSS) test:

H0: data is stationary H1: data is non-stationary

We want stationary data, thus we want to accept H0, hence p value \>0.05. (Or test statistic lies within critical values.)

```{r}
library(urca)
summary(ur.kpss(elec))
autoplot(elec)
```

The critical value at the 5% confidence interval is 0.463. Since the test statistic lies outside the critical value (7.9424\>0.463), this means that we reject the null hypothesis, hence concluding that data is non-stationary.

Let's take a look at second differencing of the seasonal differenced elec data.

```{r}
summary(ur.kpss(diff(diff((elec), 12))))
autoplot(diff(diff((elec), 12)))
```

In this case, test statistic (0.0363) \< critical value at 5% CI (0.463), test statistic lies within critical value, thus we are unable to reject H0. We conclude that data is stationary.

## 5.6 Automatic differencing

The ndiffs() function in R automatically determines order of differencing required for data, running the KPSS unit root test in the back-end.

Use nsdiffs() function for no. of determining *seasonal* differencing.

If data is seasonal, run nsdiffs() first to get the order of seasonal differencing. Then run ndiffs() on the seasonal differenced data to get the order of differencing. Let's illustrate this with code.

```{r}
# we know the elec data set is seasonal, hence we first use nsdiffs
elec |>
  nsdiffs()

# we get output of 1, meaning 1 order of seasonal differencing needed. Now we apply ndiffs to the object diff(elec,12) which is 1st order seasonal differenced elec data.
diff(elec,12)|>
  ndiffs()
# or
elec|>
  diff(12) |>
  ndiffs()

# we get output of 1, meaning 1 order of differencing is needed. Run ndiffs() again for checking purposes.
diff(diff(elec,12)) |>
  ndiffs()

# now we get output of 0, as expected :)
```

# 6 TS decomposition

Time series contains patterns of 1. Trend 2. Season 3. Cycles.

We could either:

1\. Model entire time series or

2\. Split time series to individual components (E,T,S), then merge individual models back together. This allows us to get a better matching of parameters to the underlying time series

## 6.1 Code examples

```{r}
# Additive decomposition
decompose(elec) |>
  autoplot()
```

Data = TrendCycle + Seasonal + Remainder. We usually use additive decomposition when variation of the data does not change over time

```{r}
# Multiplicative decomposition
decompose(elec, type="multiplicative")|>
  autoplot()
```

Data = TrendCycle \* Seasonal \* Remainder. We usually use multiplicative decomposition when the range of the data depends on its level, and that level might be increasing over time.

## 6.2 Decomposition Procedure

1.  Estimate trend-cycle component using MA smoothing (to remove seasonalities).

2.  Remove this component from the series.

3.  Take the output of step 2 and estimate the seasonal component by taking the average of the detrended values for each season. (eg. for 30 year monthly data, find the average value of January from 30 values of 30 years of January, repeat for Feb... and so on).

4.  Adjusted to ensure that they sum to zero (m) for the additive (multiplicative) case.

5.  String together monthly values and replicating the same sequence (copy-paste) for each year.

6.  Remainder component is the "irregular" part of the series. Additive = data - trend - seasonality, Multiplicative = data/(trend\*seasonality).

## 6.3 Moving Averages

Step 1 - Using MA to get trend-cycle component

```{r}
elec |>
  autoplot()

# 3 month moving average
elec |>
  ma(3) |>
  autoplot()

# 12 month moving average
autoplot(ma(elec,12))

# This gives us the long-term trend component, which we will remove from raw data such that we get detrended data. 
```

Step 2 - Remove this component (trend) from series

```{r}
# elec - ma(elec,12) additive decomposition
# elec / ma(elec,12) multiplicative decomposition

# additive detrended data series
autoplot(elec-ma(elec,12)) +
  ggtitle("Additive decomposition")
  

# multiplicative detrended data series
(elec/ma(elec, 12))|>
  autoplot() +
  ggtitle("Multiplicative decomposition")
```

Step 3 - Taking the average of seasonal components of each month and estimating seasonal component

```{r}
detrended_series <- elec-ma(elec,12)

cycle(detrended_series)

# get the monthly mean values over the many years of data (eg. average of Jan values using 39 years of Jan values, then Feb, and so on...)
tapply(detrended_series, cycle(detrended_series), mean, na.rm=TRUE)

plot(tapply(detrended_series, cycle(detrended_series), mean, na.rm=TRUE))
```

Step 4 - Shift seasonal component estimation so they sum to 0

```{r}
# sum of seasonal components (adding up all 12 means of each month)
sum(tapply(detrended_series, cycle(detrended_series), mean, na.rm=TRUE))

# since the sum of seasonal components is -16.1031, we should shift each point (upwards) by
+16.1031/12. 

# This is the same as the average value of the mean of each month and so is represented by the code below:
mean(tapply(detrended_series, cycle(detrended_series), mean, na.rm=TRUE))

# and so we have to shift upwards by this amount
```

Step 5 - String together monthly values (copy paste)

```{r}
# Therefore taking mean of each month of detrended series minus average sum of mean monthly data will allow us to get a seasonal component shifted to 0 

#Jan mean + 1.341, Feb mean + 1.341, March mean + 1.341, and so on...
seasonal_component <- (tapply(detrended_series, cycle(detrended_series), mean, na.rm=TRUE)) - mean(tapply(detrended_series, cycle(detrended_series), mean, na.rm=TRUE))

# This is how we manually create one seasonal cycle from the ETS decomposition plot
plot(seasonal_component, type="l")
```

Step 6 - Remainder is just raw data - trendcycle - seasonal \[additive\]

```{r}
autoplot(decompose(elec))
```

# 7 Preprosessing workflow

Workflow: 1. Remove deterministic variation (monthdays, inflation, etc.)

2.  ts decomposition -\> returns 2 things

    a\. seasonality adjusted (trend + noise)

    b\. seasonal (run naive methods)

"usually" run box-cox on 2a Model A: then run nsdiffs and ndiffs, then start to model BoxCox(2a) (Build Arima etc on this)

Model B: seasonal component -\> snaive, or can try Arima etc

Final step: (assume TS additive decomp): Take Model A + Model B = final forecast

# 8 ARIMA

AR: Auto regression -\> regression of the variable against past values of itself

I: Differencing -\> make series stationary before AR or MA steps

MA: Moving average -\> involves taking the moving average of the (first or more differenced) variable within a sliding window

## 8.1 White noise (ut) is a basic building block!

Properties of white noise:

1.  Identically distributed, stationary process
2.  E(u~t~)=0 -\> centred around zero
3.  V(u~t~) = sigma\^squared~u~ -\> constant variance
4.  C(u~t~, u~t+k~) = 0 for any k =/= 0 -\> not correlated to different time periods i.e. no autocorrelation

## 8.2 ARMA properties

### 8.2.1 AR properties

AR(1) process: 

Y~t~= c + λY~t − 1~ + u~t~

Important formula for AR(1) process:

Corr(Y~t~ , Y ~t+/-k~ ) = λ^\|k\|^ ,where λ represents AR coefficient, k represents no. of lags

Therefore, the ACF for an AR(1) process will decrease slowly but exponentially, as per the formula above.

The PACF for an AR(1) process will show a sharp drop after 1 lag. Likewise, the PACF of an AR(k) process will show a sharp drop after k lags (at the k+1 lag).

### 8.2.2 MA properties

MA(1) process:

Y~t~ = c + u~t~ + θ~1~u~t − 1~

In contrast to an AR(1) model, ACF for MA(1) cuts off after lag 1, indicating it captures very short term dynamics.

### 8.2.3 ARMA Coefficients

Stationarity depends solely on AR coefficients as MA processes are always stationary.

AR coefficients -\> determines stationarity, λ has to be within (0,1) for the series mean to be finite

MA coefficients -\> determines invertability

### 8.2.4 ARMA ACF & PACF

For an ARMA process, the ACF and the PACF should both tail off exponentially:

AR component -\> ACF decays exponentially

MA component -\> PACF decays exponentially

An MA(1) process is invertible to a AR(infinity) process, and an AR(infinity) process will have a PACF that is infinite in extent, and decaying exponentially.

One last note, if ACF decays slowly instead of exponentially, then it means that series is not stationary and differencing is necessary.

## 8.3 ARIMA models
Pls refer to slides from this point onwards...




