---
title: "time_series_learnings"
subtitle: DSA301 Time Series Data Analysis
format: html
editor: visual
---

# Load required packages

```{r}
library(tidyverse)
library(tsibble)
library(fpp2)
library(fpp3) # updated textbook
library(gridExtra)
```

# time_series_01

# 1. BoxCox Transformation

### 1.1 The BoxCox transformation is used to make variance over time as constant as possible (constant amplitude and wavelength)

```{r}
autoplot(BoxCox(elec,2))
autoplot(BoxCox(elec,10))
autoplot(BoxCox(elec,0.1))
```

### 1.2 Optimal lambda

```{r}
autoplot(BoxCox(elec, BoxCox.lambda(elec)))
BoxCox.lambda(elec)
```

# 2. Benchmark Forecasting methods

### 2.1 Naive, mean, seasonal naive and drift

Naive methods are *short-term* (eg. predicting the value of today's stock based on latest closing price).

Drift and mean are *long-term.*

```{r}
# Set training data from 1992 to 2006
train <- aus_production |>
  filter_index("1992 Q1" ~ "2006 Q4")
# Fit the models
beer_fit <- train |>
  model(
    Mean = MEAN(Beer),
    `Naïve` = NAIVE(Beer),
    `Seasonal naïve` = SNAIVE(Beer)
  )
# Generate forecasts for 14 quarters
beer_fc <- beer_fit |> forecast(h = 14)
# Plot forecasts against actual values
beer_fc |>
  autoplot(train, level = NULL) +
  autolayer(
    filter_index(aus_production, "2007 Q1" ~ .),
    colour = "black"
  ) +
  labs(
    y = "Megalitres",
    title = "Forecasts for quarterly beer production"
  ) +
  guides(colour = guide_legend(title = "Forecast"))


```

# 3. Ljung-Box test

Tests if time series data is captured in *residuals*, et, where et = yhat - y. If time series data is captured in residuals (et), it is not being captured in the model (yhat). Hence, we have to adjust our model to capture the time series data.

As a rule of thumb, residuals should:

1.  be *uncorrelated*. Correlation means there is information left in residuals to be modelled.

2.  have *zero mean*. Non zero mean in residuals means that fitted values are biased.

```{r}
autoplot(naive(elec,10))
```

### 3.1 Check if residuals are correlated with lagged values of itself

```{r}
forecast(naive(elec,10))
checkresiduals(forecast(naive(elec,10)))
```

### 3.2 There is seasonal data in the residuals, not captured by naive methods.

Hence, we can try seasonal naive forecasting to capture this time series data in our model.

```{r}
forecast(snaive(elec,10))
checkresiduals(forecast(snaive(elec,10)))

```

### 3.3 Seasonality has been captured in the snaive model.

However, there is positive long term trend in the data (seen in the ACF plot).

### 3.4 Ljung-Box takeaway:

A low p-value means that we reject H0 (there is no autocorrelation among lags), hence we assume that H1 is true (autocorrelation exists), failing the Ljung-Box test. Therefore, there is time series information still not captured in the model (since autocorrelation still exists in the **white noise**, which should instead be captured in the **model**)

Basically, if p value is \>0.05, time series model is good enough. if p value \<0.05, the time series model is still inadequate, there is time series data in the residuals yet to be captured by model

**Intuition: h0 = there is no autocorrelation in residuals. We want to accept h0, hence p value must be \>0.05**

# time_series_02

# 4. ACF

Using ACF to infer time series patterns (Is there seasonality? Trend?)

Let's look at Australian monthly electricity production data. There is clear trend and seasonality.

```{r}
autoplot(elec)
```

Lets narrow it down to after 1980s, and get the ACF plot.

```{r}
window(elec, start=1980)
ggAcf(window(elec, start=1980), lag=48)
```

## 4.1 How to read ACF plot

The second line represents the autocorrelation between Yt and Yt-1, third line represents autocorrelation between Yt and Yt-2... and so on...

In R, the first line represents autocorrelation of Yt with itself. Naturally, this is 1 (since anything correlated to itself is 1).

## 4.2 What could the ACF plot tell us?

From this acf plot, we are able to infer that the time series has:

**a trend component**: The acf displays a slowly decaying pattern.

Autocorrelations for small lags tend to be large and positive because observations nearby in time are similar in size. Hence, the ACF of trended time series tend to have positive values that slowly decrease as lags increase.

**a seasonal component**: There will be peaks in the acf at the relevant seasonal lags (in this case, since the data is monthly, there are peaks at 12, 24, 36 and 48)

# 5. Stationarity

1.  A stationary time series is one whose moments (1st moment = mean, 2nd moment = variance) do not depend on the time of observation.

2.  A less stringent definition: *Covariance stationarity* - Only the first 2 moments and autocorrelation function are independent of time t, but may still vary with lag interval k.

## 5.1 Why do we wish to estimate models on stationary data?

A stationary data has **parameters which remain constant**. Hence, model's parameters at time t will be good predictor for values after time t.

1.  Is a ts with trend covariance stationary? No, expected values will increase/decrease over time.
2.  Is a white noise ts stationary? Yes, a white noise generally has a mean of 0 and constant variance.
3.  How about a ts with seasonality? No, expected value varies according to month/quarter.
4.  How about a ts with non annual cycles? No, it also has a conditional expectation that varies over time of year.

Takeaway: Stationary data does not have trend or seasonality.

How to make data stationary? Use differencing!

## 5.2 Differencing

### 5.2.1 First differencing

y' = y~t~ - y~t-1~ which returns a series with T-1 values.

diff() function in R

### 5.2.2 Second differencing

y''~t~ = y'~t~ - y'~t-1~ = (y~t~-y~t-1~)-(y~t-1~-y~t-2~) = y~t~-2y~t-1~+y~t-2~ returns a series with T-2 values

diff(diff()) nested function in R

### 5.2.3 Order of integration

If a non-stationary ts is differenced d times before it becomes stationary, then it is integrated of order d!

### 5.2.4 Seasonal differencing

d~m~yt' = yt - y~t-m,~ where m is number of seasons

aka *lag-m differences* eg. m = 4 for quarterly data

diff(,4) in R

## 5.3 Example using elec data

```{r}
autoplot(elec)
```

This is clearly not covariance stationary, lets diff it!

```{r}
autoplot(diff(elec))
```

First difference is still not covariance stationary, mean appears to be 0 but variance increases over time.

Let's try seasonal differencing (since elec data is seasonal anyway).

```{r}
autoplot(diff(elec, 12))
```

Seasonal diff looks good, variance is more stable. But we could do better...

```{r}
autoplot(diff(log(elec),12))

```

Indeed, seasonal diff on log transformed elec is better, but is it stationary?

```{r}
acf((diff(log(elec),12)))

```

Not really stationary... There is a slow drop in acf with lag (suggestive of a trend). For stationarity, ACF would drop to zero relatively quickly.

Lastly, lets try differencing the seasonal differenced log(elec) data.

```{r}
autoplot(diff((diff(log(elec),12))))
acf(diff((diff(log(elec),12))))

```

This is stationary! Time series autoplot is constant in the first and second moments while ACF demonstrates sharp drop.

Takeaway: if data is seasonal, don't do first differencing, just do seasonal differencing first. Also, applying transformations such as logarithms may improve model performance.

## 5.4 Pipeline

1.  Check if a variable is stationary.
2.  If not, apply difference.
3.  Loop and evaluate differenced variable until it passes 'stationarity'.

Note that in reality, we do not difference more than 2 times, as doing so leads to the model losing its economic intuition.

## 5.5 Statistical test for stationarity

Kwiatkowski–Phillips–Schmidt–Shin (KPSS) test:

H0: data is stationary H1: data is non-stationary

We want stationary data, thus we want to accept H0, hence p value \>0.05. Or test statistic lies within critical values.

```{r}
library(urca)
summary(ur.kpss(elec))
autoplot(elec)
```

The critical value at the 5% confidence interval is 0.463. Since the test statistic lies outside the critical value (7.9424\>0.463), this means that we reject the null hypothesis, hence concluding that data is non-stationary.

Let's take a look at second differencing of the seasonal differenced elec data.

```{r}
summary(ur.kpss(diff(diff((elec), 12))))
autoplot(diff(diff((elec), 12)))
```

In this case, test statistic (0.0363) \< critical value at 5% CI (0.463), test statistic lies within critical value, thus we are unable to reject H0. We conclude that data is stationary.

## 5.6 Automatic differencing

The ndiffs() function in R automatically determines order of differencing required for data, running the KPSS unit root test in the back-end.

Use nsdiffs() function for no. of determining *seasonal* differencing.

If data is seasonal, run nsdiffs() first to get the order of seasonal differencing. Then run ndiffs on the seasonal differenced data to get the order of differencing. Let's illustrate this with code.

```{r}
# we know the elec data set is seasonal, hence we first use nsdiffs
elec |>
  nsdiffs()

# we get output of 1, meaning 1 order of seasonal differencing needed. Now we apply ndiffs to the object diff(elec,12) which is 1st order seasonal differenced elec data.
diff(elec,12)|>
  ndiffs()

# we get output of 1, meaning 1 order of differencing is needed. Run ndiffs() again for checking purposes.
diff(diff(elec,12)) |>
  ndiffs()

# now we get output of 0, as expected :)
```
# 6.0 TS decomposition